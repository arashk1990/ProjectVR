{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "\n",
    "sys.path.append('/home/arash/ProjectVR/')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.animation as animation\n",
    "import deepsurv\n",
    "import json\n",
    "import theano.tensor\n",
    "import keras.backend as K\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "from deepsurv import deep_surv, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json model: {\"n_in\": 19, \"learning_rate\": 0.0008208386784744157, \"hidden_layers_sizes\": [90, 90], \"lr_decay\": 0.000551240234375, \"activation\": \"rectify\", \"dropout\": 0.0, \"batch_norm\": true, \"standardize\": false}\n"
     ]
    }
   ],
   "source": [
    "#deepsurv formatted data    \n",
    "with open('/home/arash/ProjectVR/cleaneddata/deepwaitdata/xtest', 'rb') as f:\n",
    "    xtest=pickle.load(f)\n",
    "\n",
    "with open('/home/arash/ProjectVR/cleaneddata/deepwaitdata/xtrain', 'rb') as f:\n",
    "    xtrain=pickle.load(f)\n",
    "    \n",
    "with open('/home/arash/ProjectVR/cleaneddata/deepwaitdata/ytrain', 'rb') as f:\n",
    "    ytrain=pickle.load(f)  \n",
    "    \n",
    "with open('/home/arash/ProjectVR/cleaneddata/deepwaitdata/ytest', 'rb') as f:\n",
    "    ytest=pickle.load(f) \n",
    "    \n",
    "network=deep_surv.load_model_from_json(model_fp = '/home/arash/ProjectVR/models/Deepwait100epochsTuned',\n",
    "                     weights_fp = '/home/arash/ProjectVR/models/deepwait_weights_100_epochs.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = network.predict_risk(xtrain[:,0:network.hyperparams['n_in']]) #calculate logpartial hazard \n",
    "## Modify risks to have a zero mean\n",
    "modifiedrisks = risks - risks.mean()\n",
    "parthazards = np.exp(modifiedrisks)   #calculate partial hazard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [np.full((1832,1), 0.01, dtype='float32')]\n",
    "data.append(parthazards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaitPred(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, nodes=10, batch_size=64, steps_in=1, steps_out=10,callbacks=[history],\n",
    "                 epochs=200, layers=1\n",
    "                 ):\n",
    "        self.nodes = nodes\n",
    "        self.batch_size = batch_size\n",
    "        #self.layers = layers\n",
    "        self.steps_in = steps_in\n",
    "        self.steps_out = steps_out\n",
    "        self.layers = layers\n",
    "        self.epochs = epochs\n",
    "        self.callbacks= history\n",
    "        \n",
    "        \n",
    "    def custom_loss(self,y_true,y_pred):\n",
    "        \n",
    "        loss = K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "        return loss\n",
    "        \n",
    "\n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        fakeinput = Input(shape=(self.steps_in,), dtype='float32', name='fake_input')\n",
    "        \n",
    "        Hlayer = Dense(self.nodes, activation='relu')(fakeinput)\n",
    "        \n",
    "        if self.layers>1:\n",
    "            for i in range(self.layers-1):\n",
    "                Hlayer = Dense(self.nodes,activation='relu')(Hlayer)\n",
    "                Hlayer = Dropout(0.2)(Hlayer)\n",
    "                \n",
    "\n",
    "            \n",
    "        Surv0Layer = Dense(self.steps_out, activation='sigmoid')(Hlayer)\n",
    "\n",
    "        partialH_input = Input(shape=(1,), name='partialH')\n",
    "\n",
    "        surv_layer = Lambda(lambda a: tf.keras.backend.pow\n",
    "                            (a, tf.dtypes.cast(partialH_input, tf.float32)))(Surv0Layer)\n",
    "\n",
    "\n",
    "        output = Lambda(lambda a : tf.dtypes.cast\n",
    "                        (tf.keras.backend.sum(a),tf.float32))(surv_layer)\n",
    "\n",
    "        modelwait = Model(inputs=[fakeinput, partialH_input], outputs=output)\n",
    "\n",
    "\n",
    "        modelwait.compile(optimizer='adam', loss = self.custom_loss)\n",
    "        return modelwait\n",
    "    \n",
    "    def fit(self,data,y):\n",
    "        start = time.time()\n",
    "        fakeinput = data[0]\n",
    "        partialH_input = data[1]    \n",
    "        self.model=self.create_model()\n",
    "        self.model.fit([fakeinput,partialH_input], [y], epochs = self.epochs, \n",
    "                        batch_size = self.batch_size)\n",
    "        end = time.time()\n",
    "        print (\"Finished Fitting Model. # of Epochs: %d\\n Time Taken : %d secs\"\n",
    "               % (self.epochs,end - start))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        fakeinput = inp[0]\n",
    "        partialH_input = inp[1] \n",
    "        \n",
    "        return self.model.predict([fakeinput,partialH_input])\n",
    "    \n",
    "    def score(self,inp,y):\n",
    "        fakeinput = inp[0]\n",
    "        partialH_input = inp[1]     \n",
    "        ypred = self.predict(inp)\n",
    "        \n",
    "        rmseScore = -sqrt(mean_squared_error(ypred, y))    #negative of RMSE\n",
    "        return rmseScore\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wtmodel = WaitPred(epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1832/1832 [==============================] - 2s 1ms/step - loss: 292.3389\n",
      "Epoch 2/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 287.6359\n",
      "Epoch 3/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 281.0884\n",
      "Epoch 4/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 273.3016\n",
      "Epoch 5/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 264.3673\n",
      "Epoch 6/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 253.9140\n",
      "Epoch 7/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 242.3408\n",
      "Epoch 8/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 229.5482\n",
      "Epoch 9/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: 216.9596\n",
      "Epoch 10/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 203.7388\n",
      "Epoch 11/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 190.6802\n",
      "Epoch 12/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 177.4957\n",
      "Epoch 13/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 165.0590\n",
      "Epoch 14/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 153.8436\n",
      "Epoch 15/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 142.8332\n",
      "Epoch 16/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 132.1366\n",
      "Epoch 17/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 122.8411\n",
      "Epoch 18/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 113.8288\n",
      "Epoch 19/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 106.6016\n",
      "Epoch 20/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 99.3937\n",
      "Epoch 21/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: 92.6426\n",
      "Epoch 22/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: 86.7629\n",
      "Epoch 23/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: 81.6862\n",
      "Epoch 24/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: 76.5739\n",
      "Epoch 25/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 72.1703\n",
      "Epoch 26/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 68.3595\n",
      "Epoch 27/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 64.3130\n",
      "Epoch 28/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 61.2769\n",
      "Epoch 29/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 58.0955\n",
      "Epoch 30/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 55.1802\n",
      "Epoch 31/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 52.5005\n",
      "Epoch 32/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 50.3006\n",
      "Epoch 33/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: 48.2928\n",
      "Epoch 34/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 45.9035\n",
      "Epoch 35/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 44.1306\n",
      "Epoch 36/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 42.4918\n",
      "Epoch 37/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 40.7212\n",
      "Epoch 38/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 39.2381\n",
      "Epoch 39/200\n",
      "1832/1832 [==============================] - 0s 27us/step - loss: 37.9147\n",
      "Epoch 40/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 36.5076\n",
      "Epoch 41/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 35.1331\n",
      "Epoch 42/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: 34.0713\n",
      "Epoch 43/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 33.1048\n",
      "Epoch 44/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 31.9621\n",
      "Epoch 45/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 30.8996\n",
      "Epoch 46/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 30.0314\n",
      "Epoch 47/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 28.9022\n",
      "Epoch 48/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 28.3419\n",
      "Epoch 49/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 27.6029\n",
      "Epoch 50/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 26.7402\n",
      "Epoch 51/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 25.8409\n",
      "Epoch 52/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 25.2788\n",
      "Epoch 53/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 24.6903\n",
      "Epoch 54/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 23.9811\n",
      "Epoch 55/200\n",
      "1832/1832 [==============================] - 0s 28us/step - loss: 23.5553\n",
      "Epoch 56/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: 22.9807\n",
      "Epoch 57/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: 22.4873\n",
      "Epoch 58/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: 21.8748\n",
      "Epoch 59/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 21.3056\n",
      "Epoch 60/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 21.0992\n",
      "Epoch 61/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 20.6553\n",
      "Epoch 62/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 20.1375\n",
      "Epoch 63/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 19.5916\n",
      "Epoch 64/200\n",
      "1832/1832 [==============================] - 0s 18us/step - loss: 19.4491\n",
      "Epoch 65/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 18.8246\n",
      "Epoch 66/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 18.7880\n",
      "Epoch 67/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 18.1979\n",
      "Epoch 68/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 17.8382\n",
      "Epoch 69/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: 17.6438\n",
      "Epoch 70/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 17.4403\n",
      "Epoch 71/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: 17.2556\n",
      "Epoch 72/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 16.7841\n",
      "Epoch 73/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: 16.4720\n",
      "Epoch 74/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 16.3466\n",
      "Epoch 75/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 16.2661\n",
      "Epoch 76/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 15.6214\n",
      "Epoch 77/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 15.2794\n",
      "Epoch 78/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: 15.4635\n",
      "Epoch 79/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 15.2556\n",
      "Epoch 80/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 14.9317\n",
      "Epoch 81/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 14.5350\n",
      "Epoch 82/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 14.6090\n",
      "Epoch 83/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 14.7121\n",
      "Epoch 84/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 14.2261\n",
      "Epoch 85/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 14.0500\n",
      "Epoch 86/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 13.8140\n",
      "Epoch 87/200\n",
      "1832/1832 [==============================] - 0s 27us/step - loss: 13.8326\n",
      "Epoch 88/200\n",
      "1832/1832 [==============================] - 0s 28us/step - loss: 13.5795\n",
      "Epoch 89/200\n",
      "1832/1832 [==============================] - 0s 27us/step - loss: 13.7545\n",
      "Epoch 90/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 13.2877\n",
      "Epoch 91/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 13.0383\n",
      "Epoch 92/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 13.3856\n",
      "Epoch 93/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 13.1507\n",
      "Epoch 94/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 12.7728\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832/1832 [==============================] - 0s 26us/step - loss: 13.0681\n",
      "Epoch 96/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 12.3820\n",
      "Epoch 97/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: 12.4873\n",
      "Epoch 98/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 12.5855\n",
      "Epoch 99/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 12.3016\n",
      "Epoch 100/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 12.1035\n",
      "Epoch 101/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 12.2396\n",
      "Epoch 102/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 12.2014\n",
      "Epoch 103/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 12.1444\n",
      "Epoch 104/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 12.0525\n",
      "Epoch 105/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 11.9296\n",
      "Epoch 106/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 11.8401\n",
      "Epoch 107/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 11.8934\n",
      "Epoch 108/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 11.4483\n",
      "Epoch 109/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 11.6357\n",
      "Epoch 110/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 11.6077\n",
      "Epoch 111/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 11.4650\n",
      "Epoch 112/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 11.8937\n",
      "Epoch 113/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 11.2208\n",
      "Epoch 114/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 11.3931\n",
      "Epoch 115/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 11.2890\n",
      "Epoch 116/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 11.1468\n",
      "Epoch 117/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 11.0813\n",
      "Epoch 118/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 10.9456\n",
      "Epoch 119/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 11.2715\n",
      "Epoch 120/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 11.0944\n",
      "Epoch 121/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 10.9119\n",
      "Epoch 122/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 10.9438\n",
      "Epoch 123/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.6478\n",
      "Epoch 124/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 10.8813\n",
      "Epoch 125/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 10.7305\n",
      "Epoch 126/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 10.9221\n",
      "Epoch 127/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 10.5125\n",
      "Epoch 128/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 10.9700\n",
      "Epoch 129/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: 10.7110\n",
      "Epoch 130/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 10.4213\n",
      "Epoch 131/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 10.5159\n",
      "Epoch 132/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 10.4674\n",
      "Epoch 133/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 10.5291\n",
      "Epoch 134/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 10.8987\n",
      "Epoch 135/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 10.7127\n",
      "Epoch 136/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 10.3263\n",
      "Epoch 137/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.5151\n",
      "Epoch 138/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 10.4260\n",
      "Epoch 139/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.6725\n",
      "Epoch 140/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.3749\n",
      "Epoch 141/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 10.1625\n",
      "Epoch 142/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.2377\n",
      "Epoch 143/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.0880\n",
      "Epoch 144/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 10.0878\n",
      "Epoch 145/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.3205\n",
      "Epoch 146/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 10.2268\n",
      "Epoch 147/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.3866\n",
      "Epoch 148/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.1985\n",
      "Epoch 149/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: 9.9619\n",
      "Epoch 150/200\n",
      "1832/1832 [==============================] - 0s 28us/step - loss: 10.1706\n",
      "Epoch 151/200\n",
      "1832/1832 [==============================] - 0s 26us/step - loss: 10.0073\n",
      "Epoch 152/200\n",
      "1832/1832 [==============================] - 0s 29us/step - loss: 10.1091\n",
      "Epoch 153/200\n",
      "1832/1832 [==============================] - 0s 27us/step - loss: 9.9680\n",
      "Epoch 154/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 9.9733\n",
      "Epoch 155/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 9.9177\n",
      "Epoch 156/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 10.0767\n",
      "Epoch 157/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 9.7679\n",
      "Epoch 158/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 9.9293\n",
      "Epoch 159/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.0573\n",
      "Epoch 160/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.7137\n",
      "Epoch 161/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 9.9641\n",
      "Epoch 162/200\n",
      "1832/1832 [==============================] - 0s 19us/step - loss: 10.0419\n",
      "Epoch 163/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.7271\n",
      "Epoch 164/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.9600\n",
      "Epoch 165/200\n",
      "1832/1832 [==============================] - 0s 18us/step - loss: 9.8001\n",
      "Epoch 166/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 9.8437\n",
      "Epoch 167/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.7008\n",
      "Epoch 168/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.9089\n",
      "Epoch 169/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.7043\n",
      "Epoch 170/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.7922\n",
      "Epoch 171/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.9267\n",
      "Epoch 172/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.8458\n",
      "Epoch 173/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.6878\n",
      "Epoch 174/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: 9.7344\n",
      "Epoch 175/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: 9.9457\n",
      "Epoch 176/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: 9.9190\n",
      "Epoch 177/200\n",
      "1832/1832 [==============================] - 0s 21us/step - loss: nan\n",
      "Epoch 178/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: nan\n",
      "Epoch 179/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: nan\n",
      "Epoch 180/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: nan\n",
      "Epoch 181/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: nan\n",
      "Epoch 182/200\n",
      "1832/1832 [==============================] - 0s 25us/step - loss: nan\n",
      "Epoch 183/200\n",
      "1832/1832 [==============================] - 0s 24us/step - loss: nan\n",
      "Epoch 184/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: nan\n",
      "Epoch 185/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: nan\n",
      "Epoch 186/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: nan\n",
      "Epoch 187/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: nan\n",
      "Epoch 188/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: nan\n",
      "Epoch 189/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832/1832 [==============================] - 0s 23us/step - loss: nan\n",
      "Epoch 190/200\n",
      "1832/1832 [==============================] - 0s 27us/step - loss: nan\n",
      "Epoch 191/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: nan\n",
      "Epoch 192/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: nan\n",
      "Epoch 193/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: nan\n",
      "Epoch 194/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: nan\n",
      "Epoch 195/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: nan\n",
      "Epoch 196/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: nan\n",
      "Epoch 197/200\n",
      "1832/1832 [==============================] - 0s 20us/step - loss: nan\n",
      "Epoch 198/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: nan\n",
      "Epoch 199/200\n",
      "1832/1832 [==============================] - 0s 22us/step - loss: nan\n",
      "Epoch 200/200\n",
      "1832/1832 [==============================] - 0s 23us/step - loss: nan\n",
      "Finished Fitting Model. # of Epochs: 200\n",
      " Time Taken : 10 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WaitPred(batch_size=64,\n",
       "     callbacks=<keras.callbacks.TerminateOnNaN object at 0x7fa424d99fd0>,\n",
       "     epochs=200, layers=1, nodes=10, steps_in=1, steps_out=10)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtmodel.fit(data,ytrain[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtmodel.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.632305919839107"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtmodel.score(data,ytrain[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa419229828>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(wtmodel.predict(data), alpha=0.5, bins = 50, rwidth=0.9, range = (0,100), label='Predicted')\n",
    "plt.hist(ytrain[:,1], alpha=0.5, bins = 50, rwidth=0.9, range = (0,100),  label='Actual')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
