{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "sys.path.append('/home/arash/ProjectVR/')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, Lambda\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.animation as animation\n",
    "import deepsurv\n",
    "import json\n",
    "import theano.tensor\n",
    "import keras.backend as K\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "from deepsurv import deep_surv, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading json model: {\"n_in\": 19, \"learning_rate\": 0.0008208386784744157, \"hidden_layers_sizes\": [90, 90], \"lr_decay\": 0.000551240234375, \"activation\": \"rectify\", \"dropout\": 0.0, \"batch_norm\": true, \"standardize\": false}\n"
     ]
    }
   ],
   "source": [
    "#deepsurv formatted data    \n",
    "with open('/home/arash/ProjectVR/cleaneddata/deepwaitdata/xtest', 'rb') as f:\n",
    "    xtest=pickle.load(f)\n",
    "\n",
    "with open('/home/arash/ProjectVR/cleaneddata/deepwaitdata/xtrain', 'rb') as f:\n",
    "    xtrain=pickle.load(f)\n",
    "    \n",
    "with open('/home/arash/ProjectVR/cleaneddata/deepwaitdata/ytrain', 'rb') as f:\n",
    "    ytrain=pickle.load(f)  \n",
    "    \n",
    "with open('/home/arash/ProjectVR/cleaneddata/deepwaitdata/ytest', 'rb') as f:\n",
    "    ytest=pickle.load(f) \n",
    "    \n",
    "network=deep_surv.load_model_from_json(model_fp = '/home/arash/ProjectVR/models/Deepwait100epochsTuned',\n",
    "                     weights_fp = '/home/arash/ProjectVR/models/deepwait_weights_100_epochs.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "risks = network.predict_risk(xtrain[:,0:network.hyperparams['n_in']]) #calculate logpartial hazard \n",
    "## Modify risks to have a zero mean\n",
    "modifiedrisks = risks - risks.mean()\n",
    "parthazards = np.exp(modifiedrisks)   #calculate partial hazard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [np.full((1832,1), 1, dtype='float32')]\n",
    "data.append(parthazards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaitPred(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, nodes=10, batch_size=32, steps_in=1, steps_out=10,\n",
    "                 epochs=200, layers=2,\n",
    "                 ):\n",
    "        self.nodes = nodes\n",
    "        self.batch_size = batch_size\n",
    "        #self.layers = layers\n",
    "        self.steps_in = steps_in\n",
    "        self.steps_out = steps_out\n",
    "        self.layers = layers\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        \n",
    "    def custom_loss(self,y_true,y_pred):\n",
    "        \n",
    "        loss = K.mean(K.square(y_pred - y_true))\n",
    "        return loss\n",
    "        \n",
    "\n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        fakeinput = Input(shape=(self.steps_in,), dtype='float32', name='fake_input')\n",
    "        \n",
    "        Hlayer = Dense(self.nodes, activation='relu')(fakeinput)\n",
    "        \n",
    "        if self.layers>1:\n",
    "            for i in range(self.layers-1):\n",
    "                Hlayer = Dense(self.nodes,activation='relu')(Hlayer)\n",
    "            \n",
    "        Surv0Layer = Dense(self.steps_out, activation='sigmoid')(Hlayer)\n",
    "\n",
    "        partialH_input = Input(shape=(1,), name='partialH')\n",
    "\n",
    "        surv_layer = Lambda(lambda a: tf.keras.backend.pow\n",
    "                            (a, tf.dtypes.cast(partialH_input, tf.float32)))(Surv0Layer)\n",
    "\n",
    "\n",
    "        output = Lambda(lambda a : tf.dtypes.cast(tf.keras.backend.sum(a),tf.float32))(surv_layer)\n",
    "\n",
    "        modelwait = Model(inputs=[fakeinput, partialH_input], outputs=output)\n",
    "\n",
    "\n",
    "        modelwait.compile(optimizer='adam', loss = self.custom_loss)\n",
    "        return modelwait\n",
    "    \n",
    "    def fit(self,data,y):\n",
    "        start = time.time()\n",
    "        fakeinput = data[0]\n",
    "        partialH_input = data[1]    \n",
    "        self.model=self.create_model()\n",
    "        self.model.fit([fakeinput,partialH_input], [y], epochs = self.epochs, \n",
    "                       verbose = 0, batch_size = self.batch_size)\n",
    "        end = time.time()\n",
    "        print (\"Finished Fitting Model. # of Epochs: %d\\n Time Taken : %d secs\"\n",
    "               % (self.epochs,end - start))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        fakeinput = inp[0]\n",
    "        partialH_input = inp[1] \n",
    "        \n",
    "        return self.model.predict([fakeinput,partialH_input])\n",
    "    \n",
    "    def score(self,inp,y):\n",
    "        fakeinput = inp[0]\n",
    "        partialH_input = inp[1]     \n",
    "        ypred = self.predict(inp)\n",
    "        \n",
    "        rmseScore = -sqrt(mean_squared_error(ypred, y))    #negative of RMSE\n",
    "        return rmseScore\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtmodel = WaitPred(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Fitting Model. # of Epochs: 100\n",
      " Time Taken : 4 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WaitPred(batch_size=32, epochs=100, layers=2, nodes=10, steps_in=1,\n",
       "     steps_out=10)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtmodel.fit(data,ytrain[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtmodel.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-170.83204800281925"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtmodel.score(data,ytrain[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(wtmodel.predict(data), alpha=0.5, bins = 50, rwidth=0.9, range = (0,100), label='Predicted')\n",
    "plt.hist(ytrain[:,1], alpha=0.5, bins = 50, rwidth=0.9, range = (0,100),  label='Actual')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
