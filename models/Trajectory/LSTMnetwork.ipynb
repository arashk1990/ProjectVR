{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for lstm network development. data preparation for lstm can be found in preprocess/seqdata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append('/home/traaka/Desktop/ProjectVR/')\n",
    "from preprocess import Seqdata    #import seqdata.py file\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf   #version 2.2\n",
    "import tensorflow.python.keras  #version 2.3.1\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import LSTM, Dense, Input, Embedding, Masking, Dropout\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as K\n",
    "from math import ceil, sqrt\n",
    "import random\n",
    "import glob,os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#import pdb\n",
    "#import shap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.base import BaseEstimator\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.externals import joblib\n",
    "#import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import talos as ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading files from drive\n",
    "folder = '/home/traaka/Desktop/ProjectVR/cleaneddata/seqdata/'\n",
    "\n",
    "#train set:\n",
    "\n",
    "with open(f'{folder}XscaledTrain', 'rb') as f:\n",
    "     XscaledTrain = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}YscaledTrain', 'rb') as f:\n",
    "     YscaledTrain = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}distscaledTrain', 'rb') as f:\n",
    "    distscaledTrain = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}usero1trajTrain', 'rb') as f:\n",
    "     usero1trajTrain = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}usero2trajTrain', 'rb') as f:\n",
    "     usero2trajTrain = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}usero3trajTrain', 'rb') as f:\n",
    "     usero3trajTrain = pickle.load(f)\n",
    "        \n",
    "with open(f'{folder}userauxTrain', 'rb') as f:\n",
    "     userauxTrain = pickle.load(f)\n",
    "\n",
    "#test set:\n",
    "\n",
    "with open(f'{folder}XscaledTest', 'rb') as f:\n",
    "     XscaledTest = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}YscaledTest', 'rb') as f:\n",
    "     YscaledTest = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}distscaledTest', 'rb') as f:\n",
    "    distscaledTest = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}usero1trajTest', 'rb') as f:\n",
    "     usero1trajTest = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}usero2trajTest', 'rb') as f:\n",
    "     usero2trajTest = pickle.load(f)\n",
    "\n",
    "with open(f'{folder}usero3trajTest', 'rb') as f:\n",
    "     usero3trajTest = pickle.load(f)\n",
    "        \n",
    "with open(f'{folder}userauxTest', 'rb') as f:\n",
    "     userauxTest = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypter Parameter dict\n",
    "p = {\"Llayers\" : [1,2,3],\n",
    "      \"nodes\" : [10,50,100],\n",
    "      \"batch_size\": [32,128]}\n",
    "\n",
    "# Define custom metric\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    return -K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "def VanillaLSTM(X_t,Out_t,X_val,Out_val,params):\n",
    "\n",
    "    n_features = 2 #X_t.shape[2] - 1\n",
    "    n_steps_in = X_t.shape[1]\n",
    "    n_steps_out = Out_t.shape[1]\n",
    "    \n",
    "    Llayers = params['Llayers']\n",
    "    nodes = params['nodes']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=-0.01, input_shape=(n_steps_in, n_features)))\n",
    "    \n",
    "    for i in range(Llayers-1):\n",
    "        model.add(LSTM(nodes, activation='relu',\n",
    "                       return_sequences=True))\n",
    "        \n",
    "    model.add(LSTM(nodes, activation='relu'))\n",
    "    model.add(Dense(n_steps_out))\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics = [score])\n",
    "    \n",
    "    X_t = X_t[:,:,:n_features]\n",
    "    X_val = X_val[:,:,:n_features]\n",
    "    Out_t = Out_t[:,:,0] # 0 is index for X\n",
    "    Out_val = Out_val[:,:,0]\n",
    "    \n",
    "    start = time.time()\n",
    "    history = model.fit(X_t, Out_t,\n",
    "                        validation_data=(X_val, Out_val),\n",
    "                        epochs = 100, verbose = 100,\n",
    "                        batch_size = batch_size)\n",
    "    end = time.time()\n",
    "\n",
    "    #print (f\"Finished Fitting Model.\\n Time Taken : {end - start} secs\")\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "# Evaluate the model and predict\n",
    "def RMSEerror(inputseq,Out):\n",
    "    maxXY=654.9\n",
    "    minXY=650.1\n",
    "    n_features = inputseq.shape[2] - 1\n",
    "    inputseq = inputseq[:,:,:n_features]\n",
    "    Out = Out[:,:,0] # 0 is index for X\n",
    "\n",
    "    Out_pred = VLSTM_object.best_model(metric = 'val_score').predict(inputseq)\n",
    "    \n",
    "    outinv = np.zeros(Out.shape)\n",
    "    outpredinv = np.zeros(Out_pred.shape)\n",
    "    for i in range(len(Out)):\n",
    "        pos = sum (n>=0 for n in Out[i])     #position of the last actual value on y, not padded\n",
    "        outinv[i][:pos]= Out[i][:pos] * (maxXY-minXY) + minXY\n",
    "        outpredinv[i][:pos] = Out_pred[i][:pos]* (maxXY-minXY) + minXY\n",
    "\n",
    "    RMSEerror = sqrt(mean_squared_error(outpredinv, outinv))    #negative of RMSE\n",
    "\n",
    "    return RMSEerror\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the revision, the reviewer has requested to use different combinations\n",
    "# of time series data as input. time series data are X[::n_features] the order is:\n",
    "# X,Y,O1,O2,O3,Dist (defiend in Seqdata.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning VLSTM for L_3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.36s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "AllVLSTM = pd.DataFrame()\n",
    "#data prepare\n",
    "for Dtype in ['L_3','L_5','L_7',\n",
    "             'T_1_1','T_1_2','T_2_1']:\n",
    "    \n",
    "    print (f'Scanning VLSTM for {Dtype}:')\n",
    "    funclean = Seqdata.PrepareData(dtype = Dtype)\n",
    "    inputseqTrain, outTrain = funclean.run(XscaledTrain,YscaledTrain,\n",
    "                                           distscaledTrain,usero1trajTrain,\n",
    "                                           usero2trajTrain,usero3trajTrain,\n",
    "                                           userauxTrain)\n",
    "\n",
    "    inputseqTest, outTest = funclean.run(XscaledTest,YscaledTest,\n",
    "                                         distscaledTest,usero1trajTest,\n",
    "                                         usero2trajTest,usero3trajTest,\n",
    "                                         userauxTest)\n",
    "   \n",
    "    \n",
    "    VLSTM_object = ta.Scan(x = inputseqTrain,\n",
    "                           y = outTrain, \n",
    "                           params = p,\n",
    "                           model = VanillaLSTM,\n",
    "                           val_split = 0.2,\n",
    "                           experiment_name=f'VLSTM{Dtype}')\n",
    "    \n",
    "    datadf = VLSTM_object.data\n",
    "    datadf['Dtype'] = Dtype\n",
    "    AllVLSTM = AllVLSTM.append(datadf)\n",
    "    \n",
    "    #TestError = RMSEerror(inputseqTest,outTest)\n",
    "#     analyze_object = ta.Analyze(VLSTM_object)\n",
    "#     bestparams = analyze_object.best_params(\n",
    "#         'val_score', ['round_epochs', 'loss',\n",
    "#                       'val_loss','score'])[0][:3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllVLSTM.to_csv('HyperParam_VLSTMtst.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllVLSTM.sort_values(by = 'val_score',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Scan object as input\n",
    "analyze_object = ta.Analyze(VLSTM_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the dataframe with the results\n",
    "analyze_object.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the highest score\n",
    "analyze_object.high('val_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_object.best_params('val_score', ['round_epochs', 'loss', 'val_loss','score'])[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation for hyperparameters against a metric\n",
    "analyze_object.correlate('val_loss', ['val_acc','loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot\n",
    "analyze_object.plot_line('val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up to two dimensional kernel density estimator\n",
    "analyze_object.plot_kde('val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation for hyperparameters against a metric\n",
    "analyze_object.correlate('val_loss', ['val_acc', 'loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap correlation\n",
    "analyze_object.plot_corr('val_loss', ['val_acc', 'loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a four dimensional bar grid\n",
    "analyze_object.plot_bars('batch_size', 'val_acc', 'nodes', 'Llayers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VanillaLSTM(BaseEstimator):\n",
    "    \n",
    "#     def __init__(self, nodes=50, batch_size=128,\n",
    "#                  epochs=200, Llayers=1, features=6, steps_in=0, steps_out=0\n",
    "#                  ):\n",
    "\n",
    "#         self.nodes = nodes\n",
    "#         self.batch_size = batch_size\n",
    "#         self.epochs = epochs\n",
    "#         self.Llayers= Llayers\n",
    "#         self.features = features\n",
    "#         self.steps_in = steps_in\n",
    "#         self.steps_out = steps_out\n",
    "#         self.maxXY=654.9\n",
    "#         self.minXY=650.1\n",
    "\n",
    "\n",
    "#     def create_model(self):\n",
    "#         model = Sequential()\n",
    "#         model.add(Masking(mask_value=-0.01, input_shape=(self.steps_in, self.features)))\n",
    "#         for i in range(self.Llayers-1):\n",
    "#             model.add(LSTM(self.nodes, activation='relu', return_sequences=True))\n",
    "#         model.add(LSTM(self.nodes, activation='relu'))\n",
    "#         model.add(Dense(self.steps_out))\n",
    "#         model.compile(optimizer='adam', loss='mse')\n",
    "#         return model\n",
    "    \n",
    "#     def fit(self,inputseq,out):\n",
    "#         start = time.time()\n",
    "#         X = inputseq[:,:,:self.features]\n",
    "#         out = out[:,:,0] # 0 is index for X\n",
    "#         self.model=self.create_model()\n",
    "#         self.model.fit(X, out, epochs = self.epochs, verbose = 100, batch_size = self.batch_size)\n",
    "#         end = time.time()\n",
    "#         print (\"Finished Fitting Model. # of Epochs: %d\\n Time Taken : %d secs\"\n",
    "#                % (self.epochs,end - start))\n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, inputseq):\n",
    "#         X = inputseq[:,:,:self.features]\n",
    "        \n",
    "#         return self.model.predict(X)\n",
    "    \n",
    "#     def score(self,inputseq,out):\n",
    "        \n",
    "#         X = inputseq[:,:,:self.features]\n",
    "#         outpred = self.predict(X)\n",
    "#         out = out[:,:,0]\n",
    "#         outinv = np.zeros(out.shape)\n",
    "#         outpredinv = np.zeros(outpred.shape)\n",
    "#         for i in range(len(out)):\n",
    "#             pos = sum (n>=0 for n in out[i])     #position of the last actual value on y, not padded\n",
    "#             outinv[i][:pos]= out[i][:pos] * (self.maxXY-self.minXY) + self.minXY\n",
    "#             outpredinv[i][:pos] = outpred[i][:pos]* (self.maxXY-self.minXY) + self.minXY\n",
    "            \n",
    "#         rmseScore = -sqrt(mean_squared_error(outpredinv, outinv))    #negative of RMSE\n",
    "\n",
    "#         return rmseScore\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting Vanilla LSTM, using grid search for hypperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = inpTrain.shape[2] - 1\n",
    "# n_steps_in = inpTrain.shape[1]\n",
    "# n_steps_out = outTrain.shape[1]\n",
    "\n",
    "# VLmodel=VanillaLSTM(epochs=200, features=n_features,\n",
    "#                     steps_in=n_steps_in, steps_out=n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_params = {\"Llayers\" : [1,2],\"nodes\" : [10,50]}#,\"batch_size\": [128]}\n",
    "\n",
    "#gs = GridSearchCV(VLmodel, tuned_params,cv = 8, refit= True, n_jobs=-1,verbose = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.load('/home/arash/ProjectVR/models/trainedmodels/VLSTM1.pkl',allow_pickle=True)      \n",
    "# outTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_result = gs.fit(inpTrain,outTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f'/home/arash/ProjectVR/models/GridResults/VLSTM200_8{dtype}.npy',grid_result.cv_results_)      \n",
    "#filename: ModelnameEpochs_CV_data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestVLSTM = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(bestVLSTM, '/home/arash/ProjectVR/models/trainedmodels/VLSTM1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestVLSTM.score(inpTest,outTest)\n",
    "\n",
    "# bestVLSTM.score(inpTest,outTest)\n",
    "\n",
    "# VLmodel.fit(inpTrain,outTrain)\n",
    "\n",
    "# VLmodel.score(inpTEST,outTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = bestVLSTM.model.history.history\n",
    "# # plot history\n",
    "# plt.plot(history['loss'], label='train')\n",
    "# #pyplot.plot(history.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm model with aux variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\"Llayers\" : [2],\n",
    "     \"Dlayers\" : [3],\n",
    "     \"nodes\" : [100],\n",
    "     \"batch_size\": [32], \n",
    "     \"drpout\" : [0]}\n",
    "\n",
    "def AuxLSTM(X_t,Out_t,X_val,Out_val,params):\n",
    "    n_features = X_t.shape[2] - 1\n",
    "    aux_size = sum(X_t[:,:,n_features][0]!=-0.01)\n",
    "    n_steps_in = X_t.shape[1]\n",
    "    n_steps_out = Out_t.shape[1]\n",
    "    \n",
    "    Llayers = params['Llayers']\n",
    "    Dlayers = params['Dlayers']\n",
    "    drpout = params['drpout']\n",
    "    nodes = params['nodes']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    \n",
    "    seq_input = Input(shape=(n_steps_in, n_features), dtype='float32', name='seq_input')\n",
    "    mask = Masking(mask_value=-0.01)(seq_input)\n",
    "    \n",
    "    if Llayers==1:\n",
    "        lstm_out = LSTM(nodes, activation='relu')(mask)\n",
    "        \n",
    "    else:\n",
    "        lstm_out = LSTM(nodes,activation='relu',return_sequences=True)(mask)\n",
    "        for i in range(Llayers-2):\n",
    "            lstm_out = LSTM(nodes,activation='relu',return_sequences=True)(lstm_out)\n",
    "        lstm_out = LSTM(nodes, activation='relu')(lstm_out)\n",
    "\n",
    "    #output for lstm, corresponds to 0.2 of loss, used to smooth training and regularization:\n",
    "    sec_output = Dense(n_steps_out, activation='sigmoid' ,name='sec_output')(lstm_out)   \n",
    "\n",
    "    auxiliary_input = Input(shape=(aux_size,), name='aux_input')\n",
    "    \n",
    "\n",
    "    mrgLayer = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "    # We stack a deep densely-connected network on top\n",
    "    for i in range(Dlayers):\n",
    "        mrgLayer = Dense(nodes, activation='relu')(mrgLayer)\n",
    "        if drpout>0:\n",
    "            mrgLayer = Dropout(drpout)(mrgLayer)\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(n_steps_out, activation='sigmoid', name='main_output')(mrgLayer)\n",
    "\n",
    "    modelaux = Model(inputs=[seq_input, auxiliary_input], outputs=[main_output, sec_output])\n",
    "\n",
    "\n",
    "    modelaux.compile(optimizer='adam', loss='mse', metrics = [score],\n",
    "              loss_weights=[1., 0.2])\n",
    "    \n",
    "    \n",
    "    aux_t = X_t[:,:,n_features][:,-aux_size:]\n",
    "    aux_val = X_val[:,:,n_features][:,-aux_size:]\n",
    "    \n",
    "    X_t = X_t[:,:,:n_features]\n",
    "    X_val = X_val[:,:,:n_features]\n",
    "    \n",
    "    \n",
    "    Out_t = Out_t[:,:,0] # 0 is index for X\n",
    "    Out_val = Out_val[:,:,0]\n",
    "    \n",
    "    #start = time.time()\n",
    "    history = modelaux.fit([X_t,aux_t], [Out_t,Out_t], \n",
    "                        validation_data = [[X_val, aux_val],[Out_val,Out_val]],\n",
    "                        epochs = 100,\n",
    "                        verbose = 10, \n",
    "                        batch_size = batch_size)\n",
    "    \n",
    "    #end = time.time()\n",
    "\n",
    "    #print (f\"Finished Fitting Model.\\n Time Taken : {end - start} secs\")\n",
    "    \n",
    "    return history, modelaux\n",
    "\n",
    "# Evaluate the model and predict\n",
    "def RMSEerror(inputseq,Out):\n",
    "    maxXY=654.9\n",
    "    minXY=650.1\n",
    "    n_features = inputseq.shape[2] - 1\n",
    "    aux_size = sum(inputseq[:,:,n_features][0]!=-0.01)\n",
    "    aux = inputseq[:,:,n_features][:,-aux_size:]\n",
    "    \n",
    "    inputseq = inputseq[:,:,:n_features]\n",
    "    \n",
    "    Out = Out[:,:,0] # 0 is index for X\n",
    "\n",
    "    Out_pred = AuxLSTM_object.best_model(\n",
    "        metric = 'val_main_output_score').predict([inputseq,aux])[0]\n",
    "    \n",
    "    outinv = np.zeros(Out.shape)\n",
    "    outpredinv = np.zeros(Out_pred.shape)\n",
    "    for i in range(len(Out)):\n",
    "        pos = sum (n>=0 for n in Out[i])     #position of the last actual value on y, not padded\n",
    "        outinv[i][:pos]= Out[i][:pos] * (maxXY-minXY) + minXY\n",
    "        outpredinv[i][:pos] = Out_pred[i][:pos]* (maxXY-minXY) + minXY\n",
    "\n",
    "    RMSEerror = sqrt(mean_squared_error(outpredinv, outinv))    #negative of RMSE\n",
    "\n",
    "    return RMSEerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEerror(inputseqTrain,outTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### T_1_1\n",
    "def score(y_true, y_pred):\n",
    "    return -K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "AllAuxLSTM = pd.DataFrame()\n",
    "#data prepare\n",
    "for Dtype in ['T_1_1']:\n",
    "    \n",
    "    print (f'Scanning AuxLSTM for {Dtype}:')\n",
    "    funclean = Seqdata.PrepareData(dtype = Dtype)\n",
    "    inputseqTrain, outTrain = funclean.run(XscaledTrain,YscaledTrain,\n",
    "                                           distscaledTrain,usero1trajTrain,\n",
    "                                           usero2trajTrain,usero3trajTrain,\n",
    "                                           userauxTrain)\n",
    "\n",
    "    inputseqTest, outTest = funclean.run(XscaledTest,YscaledTest,\n",
    "                                         distscaledTest,usero1trajTest,\n",
    "                                         usero2trajTest,usero3trajTest,\n",
    "                                         userauxTest)\n",
    "    \n",
    "    AuxLSTM_object = ta.Scan(x = inputseqTrain,\n",
    "                           y = outTrain, \n",
    "                           params = p,\n",
    "                           model = AuxLSTM,\n",
    "                           val_split = 0.2,\n",
    "                           experiment_name=f'AuxLSTM{Dtype}')\n",
    "    \n",
    "    datadf = AuxLSTM_object.data\n",
    "    datadf['Dtype'] = Dtype\n",
    "    AllAuxLSTM = AllAuxLSTM.append(datadf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestm = AuxLSTM_object.best_model(\n",
    "        metric = 'val_main_output_score')\n",
    "# Evaluate the model and predict\n",
    "funclean = Seqdata.PrepareData(dtype = 'T_1_1')\n",
    "\n",
    "inputseqTrain, outTrain = funclean.run(XscaledTrain,YscaledTrain,\n",
    "                                       distscaledTrain,usero1trajTrain,\n",
    "                                       usero2trajTrain,usero3trajTrain,\n",
    "                                       userauxTrain)\n",
    "\n",
    "inputseqTest, outTest = funclean.run(XscaledTest,YscaledTest,\n",
    "                                     distscaledTest,usero1trajTest,\n",
    "                                     usero2trajTest,usero3trajTest,\n",
    "                                     userauxTest)\n",
    "\n",
    "\n",
    "n_features = 6\n",
    "aux_size = 14\n",
    "aux = inputseqTrain[:,:,n_features][:,-aux_size:]\n",
    "inputseq = inputseqTrain[:,:,:n_features][0].reshape(1,14,6)\n",
    "Out = outTrain[:,:,0][0].reshape(1,10) # 0 is index for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(aux):\n",
    "    maxXY=654.9\n",
    "    minXY=650.1\n",
    "\n",
    "    Out_pred = bestm.predict([inputseq,np.array(aux).reshape(1,14)])[0]\n",
    "    \n",
    "    outinv = np.zeros(Out.shape)\n",
    "    outpredinv = np.zeros(Out_pred.shape)\n",
    "    #for i in range(len(Out)):\n",
    "    pos = 10     #position of the last actual value on y, not padded\n",
    "    outinv[:pos]= Out[:pos] * (maxXY-minXY) + minXY\n",
    "    outpredinv[:pos] = Out_pred[:pos]* (maxXY-minXY) + minXY\n",
    "\n",
    "    RMSEerror = sqrt(mean_squared_error(outpredinv, outinv))   \n",
    "    y = np.array([RMSEerror])\n",
    "\n",
    "    return y\n",
    "\n",
    "def f(X):\n",
    "    pr = []\n",
    "    for i in range(X.shape[0]):\n",
    "        pr.append(pred([X[i,:]]))\n",
    "    return np.array(pr).flatten()\n",
    "\n",
    "#ss = f(aux[1:3])\n",
    "\n",
    "### Interpretability\n",
    "\n",
    "#using train set for interpretability\n",
    "dfxtrain = pd.DataFrame(aux)\n",
    "dfxtrain.columns=dfxtrain.columns=['Snowy','Night', 'One way', 'two way', 'Two way with median',\n",
    "                 'Speed Limit_30.0', 'Speed Limit_40.0', 'Speed Limit_50.0',\n",
    "                 'Lane Width_2.5', 'Lane Width_2.75', 'Lane Width_3.0',\n",
    "                 'Mean Arrival Rate_530.0', 'Mean Arrival Rate_750.0',\n",
    "                 'Mean Arrival Rate_1100.0']\n",
    "\n",
    "#pred(dfxtrain)\n",
    "#yp = pred(inputseqTrain,outTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using zeros as background data for binary variables, and mean value for cont variables\n",
    "backgrounddata = pd.DataFrame(np.zeros(14)).T  \n",
    "backgrounddata.columns = dfxtrain.columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data = dfxtrain.iloc[np.random.choice(np.arange(len(dfxtrain)), 10, False)]\n",
    "\n",
    "\n",
    "explainer = shap.KernelExplainer(f,backgrounddata)\n",
    "\n",
    "shap_values = explainer.shap_values(dfxtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = shap.summary_plot(shap_values, dfxtrain,max_display=20,show =False)\n",
    "plt.savefig('shaptraj', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifSHAP = pd.DataFrame(shap_values)\n",
    "modifSHAP.columns = dfxtrain.columns\n",
    "modifSHAP = modifSHAP.drop(columns = ['Lane Width_2.5','Speed Limit_50.0',\n",
    "                          'two way','Mean Arrival Rate_1100.0'])\n",
    "\n",
    "modifdfxtrain = dfxtrain.drop(columns = ['Lane Width_2.5','Speed Limit_50.0',\n",
    "                          'two way','Mean Arrival Rate_1100.0'])\n",
    "\n",
    "fig2 = shap.summary_plot(np.array(modifSHAP), modifdfxtrain,max_display=20,show =False)\n",
    "plt.savefig('shaptraj2', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### t12\n",
    "p = {\"Llayers\" : [3],\n",
    "     \"Dlayers\" : [1],\n",
    "     \"nodes\" : [50],\n",
    "     \"batch_size\": [32], \n",
    "     \"drpout\" : [0]}\n",
    "def score(y_true, y_pred):\n",
    "    return -K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "#AllAuxLSTM = pd.DataFrame()\n",
    "#data prepare\n",
    "for Dtype in ['T_1_2']:\n",
    "    \n",
    "    print (f'Scanning AuxLSTM for {Dtype}:')\n",
    "    funclean = Seqdata.PrepareData(dtype = Dtype)\n",
    "    inputseqTrain, outTrain = funclean.run(XscaledTrain,YscaledTrain,\n",
    "                                           distscaledTrain,usero1trajTrain,\n",
    "                                           usero2trajTrain,usero3trajTrain,\n",
    "                                           userauxTrain)\n",
    "\n",
    "    inputseqTest, outTest = funclean.run(XscaledTest,YscaledTest,\n",
    "                                         distscaledTest,usero1trajTest,\n",
    "                                         usero2trajTest,usero3trajTest,\n",
    "                                         userauxTest)\n",
    "    \n",
    "    AuxLSTM_object12 = ta.Scan(x = inputseqTrain,\n",
    "                           y = outTrain, \n",
    "                           params = p,\n",
    "                           model = AuxLSTM,\n",
    "                           val_split = 0.2,\n",
    "                           experiment_name=f'AuxLSTM{Dtype}')\n",
    "    \n",
    "    #datadf = AuxLSTM_object.data\n",
    "    #datadf['Dtype'] = Dtype\n",
    "    #AllAuxLSTM = AllAuxLSTM.append(datadf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = AuxLSTM_object12.best_model(\n",
    "        metric = 'val_main_output_score')\n",
    "def RMSEerror(inputseq,Out):\n",
    "    maxXY=654.9\n",
    "    minXY=650.1\n",
    "    n_features = inputseq.shape[2] - 1\n",
    "    aux_size = sum(inputseq[:,:,n_features][0]!=-0.01)\n",
    "    aux = inputseq[:,:,n_features][:,-aux_size:]\n",
    "    \n",
    "    inputseq = inputseq[:,:,:n_features]\n",
    "    \n",
    "    Out = Out[:,:,0] # 0 is index for X\n",
    "\n",
    "    Out_pred = bm.predict([inputseq,aux])[0]\n",
    "    \n",
    "    outinv = np.zeros(Out.shape)\n",
    "    outpredinv = np.zeros(Out_pred.shape)\n",
    "    for i in range(len(Out)):\n",
    "        pos = sum (n>=0 for n in Out[i])     #position of the last actual value on y, not padded\n",
    "        outinv[i][:pos]= Out[i][:pos] * (maxXY-minXY) + minXY\n",
    "        outpredinv[i][:pos] = Out_pred[i][:pos]* (maxXY-minXY) + minXY\n",
    "\n",
    "    RMSEerror = sqrt(mean_squared_error(outpredinv, outinv))    #negative of RMSE\n",
    "\n",
    "    return RMSEerror\n",
    "RMSEerror(inputseqTest,outTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model and predict\n",
    "#funclean = Seqdata.PrepareData(dtype = 'T_1_1')\n",
    "\n",
    "#inputseqTrain, outTrain = funclean.run(XscaledTrain,YscaledTrain,\n",
    "#                                        distscaledTrain,usero1trajTrain,\n",
    "#                                        usero2trajTrain,usero3trajTrain,\n",
    "#                                        userauxTrain)\n",
    "\n",
    "# inputseqTest, outTest = funclean.run(XscaledTest,YscaledTest,\n",
    "#                                      distscaledTest,usero1trajTest,\n",
    "#                                      usero2trajTest,usero3trajTest,\n",
    "#                                      userauxTest)\n",
    "\n",
    "\n",
    "n_features = 6\n",
    "aux_size = 14\n",
    "aux = inputseqTrain[:,:,n_features][:,-aux_size:]\n",
    "inputseq = inputseqTrain[:,:,:n_features][0].reshape(1,14,6)\n",
    "Out = outTrain[:,:,0][0].reshape(1,20) # 0 is index for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(aux):\n",
    "    maxXY=654.9\n",
    "    minXY=650.1\n",
    "\n",
    "    Out_pred = bm.predict([inputseq,np.array(aux).reshape(1,14)])[0]\n",
    "    \n",
    "    outinv = np.zeros(Out.shape)\n",
    "    outpredinv = np.zeros(Out_pred.shape)\n",
    "    #for i in range(len(Out)):\n",
    "    pos = 20     #position of the last actual value on y, not padded\n",
    "    outinv[:pos]= Out[:pos] * (maxXY-minXY) + minXY\n",
    "    outpredinv[:pos] = Out_pred[:pos]* (maxXY-minXY) + minXY\n",
    "\n",
    "    RMSEerror = sqrt(mean_squared_error(outpredinv, outinv))   \n",
    "    y = np.array([RMSEerror])\n",
    "\n",
    "    return y\n",
    "\n",
    "def f(X):\n",
    "    pr = []\n",
    "    for i in range(X.shape[0]):\n",
    "        pr.append(pred([X[i,:]]))\n",
    "    return np.array(pr).flatten()\n",
    "\n",
    "#ss = f(aux[1:3])\n",
    "\n",
    "### Interpretability\n",
    "\n",
    "#using train set for interpretability\n",
    "dfxtrain = pd.DataFrame(aux)\n",
    "dfxtrain.columns=dfxtrain.columns=['Snowy','Night', 'One way', 'two way', 'Two way with median',\n",
    "                 'Speed Limit_30.0', 'Speed Limit_40.0', 'Speed Limit_50.0',\n",
    "                 'Lane Width_2.5', 'Lane Width_2.75', 'Lane Width_3.0',\n",
    "                 'Mean Arrival Rate_530.0', 'Mean Arrival Rate_750.0',\n",
    "                 'Mean Arrival Rate_1100.0']\n",
    "\n",
    "#pred(dfxtrain)\n",
    "#yp = pred(inputseqTrain,outTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using zeros as background data for binary variables, and mean value for cont variables\n",
    "backgrounddata = pd.DataFrame(np.zeros(14)).T  \n",
    "backgrounddata.columns = dfxtrain.columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data = dfxtrain.iloc[np.random.choice(np.arange(len(dfxtrain)), 10, False)]\n",
    "\n",
    "\n",
    "explainer = shap.KernelExplainer(f,backgrounddata)\n",
    "\n",
    "shap_values = explainer.shap_values(dfxtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifSHAP = pd.DataFrame(shap_values)\n",
    "modifSHAP.columns = dfxtrain.columns\n",
    "modifSHAP = modifSHAP.drop(columns = ['Lane Width_2.5','Speed Limit_50.0',\n",
    "                          'two way','Mean Arrival Rate_1100.0'])\n",
    "\n",
    "modifdfxtrain = dfxtrain.drop(columns = ['Lane Width_2.5','Speed Limit_50.0',\n",
    "                          'two way','Mean Arrival Rate_1100.0'])\n",
    "\n",
    "fig2 = shap.summary_plot(np.array(modifSHAP), modifdfxtrain,max_display=20,show =False)\n",
    "plt.savefig('shaptraj3', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllAuxLSTM.to_csv('HyperParam_AuxLSTM.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllAuxLSTM.sort_values(by = 'val_main_output_score',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AuxLSTM(BaseEstimator):\n",
    "    \n",
    "#     def __init__(self, nodes=50, batch_size=32,\n",
    "#                  epochs=200, Dlayers=1, Llayers=1, features=6, steps_in=0, steps_out=0, drpout=0.5\n",
    "#                  ):\n",
    "\n",
    "#         self.nodes = nodes\n",
    "#         self.features = features\n",
    "#         self.steps_in = steps_in\n",
    "#         self.steps_out = steps_out\n",
    "#         self.epochs = epochs\n",
    "#         self.batch_size = batch_size\n",
    "#         self.Dlayers= Dlayers   #number of hidden dense layers\n",
    "#         self.Llayers = Llayers  #number of lstm layers\n",
    "#         self.maxXY=654.9\n",
    "#         self.minXY=650.1\n",
    "#         self.drpout=drpout\n",
    "\n",
    "\n",
    "#     def create_model(self):\n",
    "        \n",
    "#         seq_input = Input(shape=(self.steps_in, self.features), dtype='float32', name='seq_input')\n",
    "#         mask = Masking(mask_value=-0.01)(seq_input)\n",
    "#         if self.Llayers==1:\n",
    "            \n",
    "#             lstm_out = LSTM(self.nodes, activation='relu')(mask)\n",
    "            \n",
    "#         else:\n",
    "#             lstm_out = LSTM(self.nodes,activation='relu',return_sequences=True)(mask)\n",
    "            \n",
    "#             for i in range(self.Llayers-2):\n",
    "#                 lstm_out = LSTM(self.nodes,activation='relu',return_sequences=True)(lstm_out)\n",
    "                \n",
    "#             lstm_out = LSTM(self.nodes, activation='relu')(lstm_out)\n",
    "\n",
    "#         #output for lstm, corresponds to 0.2 of loss, used to smooth training and regularization:\n",
    "#         sec_output = Dense(self.steps_out, activation='sigmoid' ,name='sec_output')(lstm_out)   \n",
    "\n",
    "#         auxiliary_input = Input(shape=(14,), name='aux_input')\n",
    "        \n",
    "#         x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "#         # We stack a deep densely-connected network on top\n",
    "#         for i in range(self.Dlayers):\n",
    "#             x = Dense(self.nodes, activation='relu')(x)\n",
    "#             if self.drpout>0:\n",
    "#                 x = Dropout(self.drpout)(x)\n",
    "\n",
    "#         # And finally we add the main logistic regression layer\n",
    "#         main_output = Dense(self.steps_out, activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "#         modelaux = Model(inputs=[seq_input, auxiliary_input], outputs=[main_output, sec_output])\n",
    "\n",
    "\n",
    "#         modelaux.compile(optimizer='adam', loss='mse',\n",
    "#                       loss_weights=[1., 0.2])\n",
    "#         return modelaux\n",
    "    \n",
    "#     def fit(self,inp,out):\n",
    "#         start = time.time()\n",
    "#         X = inp[:,:,:self.features]\n",
    "#         aux = inp[:,:,self.features][:,-14:] \n",
    "#         out = out[:,:,0]\n",
    "#         self.model = self.create_model()\n",
    "#         self.model.fit([X,aux], [out,out], epochs = self.epochs, verbose = 100, batch_size = self.batch_size)\n",
    "#         end = time.time()\n",
    "#         print (\"Finished Fitting AuxModel. # of Epochs: %d\\n Time Taken : %d secs\"\n",
    "#                % (self.epochs,end - start))\n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, inp):\n",
    "#         X = inp[:,:,:self.features]\n",
    "#         aux = inp[:,:,self.features][:,-14:]\n",
    "        \n",
    "#         return self.model.predict([X,aux])\n",
    "    \n",
    "#     def score(self,inp,out):\n",
    "#         out = out[:,:,0]\n",
    "#         outpred = self.predict(inp)[0]        #auxlstm has two identical outputs, 1st in chosen\n",
    "#         outinv = np.zeros(out.shape)\n",
    "#         outpredinv = np.zeros(outpred.shape)\n",
    "#         for i in range(len(out)):\n",
    "#             pos = sum (n>=0 for n in out[i])    #position of the last actual value on y, not padded\n",
    "#             outinv[i][:pos]= out[i][:pos] * (self.maxXY-self.minXY) + self.minXY\n",
    "#             outpredinv[i][:pos] = outpred[i][:pos]* (self.maxXY-self.minXY) + self.minXY\n",
    "            \n",
    "#         rmseScore = -sqrt(mean_squared_error(outpredinv, outinv))    #negative of RMSE\n",
    "#         return rmseScore\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = (inpTrain.shape[2]-1)\n",
    "# n_steps_in = inpTrain.shape[1]\n",
    "# n_steps_out = outTrain.shape[1]\n",
    "\n",
    "# Auxmodel=AuxLSTM(epochs=100, features=n_features,\n",
    "#                     steps_in=n_steps_in, steps_out=n_steps_out)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_params = {\"Dlayers\" : [1,2]}#,'drpout': [0,0.5], \"Llayers\" : [2]}\n",
    "#                # 'batch_size' : [64], 'nodes': [100]  }\n",
    "\n",
    "# gsaux = GridSearchCV(Auxmodel, tuned_params,cv = 5, refit= True, n_jobs=-1)\n",
    "\n",
    "# gridaux_result = gsaux.fit(inpTrain,outTrain)\n",
    "\n",
    "# gridaux_result.cv_results_\n",
    "\n",
    "# bestauxLSTM = gridaux_result.best_estimator_\n",
    "\n",
    "# bestauxLSTM.score(inpTest,outTest)\n",
    "\n",
    "# np.save('/home/arash/ProjectVR/models/GridResults/Aux0_5LSTM001.npy',gridaux_result.cv_results_)\n",
    "# bestauxLSTM = gridaux_result.best_estimator_\n",
    "\n",
    "# joblib.dump(bestauxLSTM, '/home/arash/ProjectVR/models/trainedmodels/Aux0_5LSTM6.pkl')\n",
    "# bestauxLSTM.score(inputseqTEST,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aux1Best=AuxLSTM(epochs=1000, features=n_features,\n",
    "#                     steps_in=n_steps_in, steps_out=n_steps_out,Llayers=2,Dlayers=1,drpout=0.5,\n",
    "#                    batch_size=32, nodes=50)\n",
    "\n",
    "# Aux2Best=AuxLSTM(epochs=100, features=n_features,\n",
    "#                     steps_in=n_steps_in, steps_out=n_steps_out,Llayers=2,Dlayers=1,drpout=0.5,\n",
    "#                    batch_size=32, nodes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestaux=joblib.load('/home/arash/ProjectVR/models/trainedmodels/AuxLSTM2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestvlstm=joblib.load('/home/arash/ProjectVR/models/trainedmodels/VLSTM1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestaux.score(inputseqtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestaux0_5=joblib.load('/home/arash/ProjectVR/models/trainedmodels/Aux0_5LSTM6.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxXY=654.9\n",
    "minXY=650.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = bestaux.predict(inputseqtrain)[0][num]        #auxlstm has two identical outputs, 1st in chosen\n",
    "ypredvlstm=bestvlstm.predict(inputseqtrain)[num] \n",
    "\n",
    "\n",
    "pos = sum (n>=0 for n in ytrain[num])                #position of the last actual value on y, not padded\n",
    "\n",
    "title= \"X Coordinate\"\n",
    "y= ypred[:pos]* (maxXY-minXY) + minXY\n",
    "x = np.arange(0,len(y)/10,0.1)\n",
    "xv=x\n",
    "yv=ypredvlstm[:pos]* (maxXY-minXY) + minXY\n",
    "\n",
    "ytrue = ytrain[num][:pos] * (maxXY-minXY) + minXYnj\n",
    "xtrue = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(xold,yold,numsteps):\n",
    "    xnew = []\n",
    "    ynew = []\n",
    "    for i in range(len(xold)-1):\n",
    "        difX = xold[i+1]-xold[i]\n",
    "        stepsX = difX/numsteps\n",
    "        difY = yold[i+1]-yold[i]\n",
    "        stepsY = difY/numsteps\n",
    "        for s in range(numsteps):\n",
    "            xnew = np.append(xnew,xold[i]+s*stepsX)\n",
    "            ynew = np.append(ynew,yold[i]+s*stepsY)\n",
    "    return xnew,ynew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=augment(x,y,10)\n",
    "xv,yv=augment(xv,yv,10)\n",
    "xtrue,ytrue=augment(xtrue,ytrue,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true0=pd.DataFrame(ytrue,xtrue)\n",
    "pred0=pd.DataFrame(y,x)\n",
    "Vpred0=pd.DataFrame(yv,xv)\n",
    "true0.columns = {title}\n",
    "\n",
    "pred0.columns={title}\n",
    "Vpred0.columns={title}\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = '/home/arash/anaconda3/envs/gpusupport/bin/ffmpeg'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=20, metadata=dict(artist='Arash'), bitrate=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.xlim(0, len(x)/100)\n",
    "plt.ylim(np.min(pred0)[0], np.max(pred0)[0])\n",
    "plt.xlabel('Time (s)',fontsize=14)\n",
    "plt.ylabel(title,fontsize=14)\n",
    "plt.title('Pedestrian Position Over Time',fontsize=14)\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color='b', lw=4),\n",
    "                Line2D([0], [0], color='g', lw=4),\n",
    "                Line2D([0], [0], color='r', lw=4)]\n",
    "plt.legend(custom_lines, ['AuxLSTM', 'True Trajectory', 'Vanilla LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    dataaux = pred0.iloc[:int(i+1)] #select data range\n",
    "    datatrue = true0.iloc[:int(i+1)] #select data range\n",
    "    dataV=Vpred0.iloc[:int(i+1)] #select data range\n",
    "    p = sns.lineplot(x=dataaux.index, y=dataaux[title], data=dataaux,markers=True,color=\"b\")\n",
    "    p2 = sns.lineplot(x=datatrue.index, y=datatrue[title], data=datatrue, markers=True,color=\"g\")\n",
    "    p3 = sns.lineplot(x=dataV.index, y=dataV[title], data=dataV, markers=True,color=\"r\")\n",
    "    sns.set_style(\"white\")\n",
    "    p.tick_params(labelsize=10)\n",
    "    p2.tick_params(labelsize=10)\n",
    "    p3.tick_params(labelsize=10)\n",
    "    plt.setp(p.lines,linewidth=2)\n",
    "    plt.setp(p2.lines,linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animation.FuncAnimation(fig, animate,frames=400 ,repeat=True)\n",
    "\n",
    "ani.save('Sample03.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
