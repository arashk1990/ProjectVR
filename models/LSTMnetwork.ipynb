{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for lstm network development. data preparation for lstm can be found in preprocess/seqdata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/arash/anaconda3/envs/gpusupport/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras\n",
    "from math import ceil\n",
    "import time\n",
    "import random\n",
    "import glob,os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pdb\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Input, Embedding, Masking\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data, not uploaded on github\n",
    "#### allauxdata columns' corresponding variables:\n",
    "###### 'Snowy', 'Night', 'One way', 'two way', 'Two way with median', 'Speed Limit_30.0', Speed Limit_40.0', 'Speed Limit_50.0', 'Lane Width_2.5', 'Lane Width_2.75', 'Lane Width_3.0', 'Mean Arrival Rate_530.0', 'Mean Arrival Rate_750.0', 'Mean Arrival Rate_1100.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "with open('/home/arash/ProjectVR/cleaneddata/seqdata', 'rb') as f:\n",
    "    allseqdata = pickle.load(f)\n",
    "\n",
    "X=allseqdata[0]\n",
    "o1=allseqdata[1]\n",
    "o2=allseqdata[2]\n",
    "o3=allseqdata[3]\n",
    "dist=allseqdata[4]\n",
    "y=allseqdata[5]\n",
    "\n",
    "#load data\n",
    "with open('/home/arash/ProjectVR/cleaneddata/auxdata', 'rb') as f:\n",
    "    allauxdata = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "#### #normalization using min max method: (keras min max not used due to format difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#o1,o2,o3: are already bw 0 and 1\n",
    "maxTraj=[]\n",
    "minTraj=[]\n",
    "mindist=[]\n",
    "for i in range(len(X)):                              #findimg max and min of each feature\n",
    "    maxTraj.append(max((max(X[i]),max(y[i]))))\n",
    "    minTraj.append(min((min(X[i]),min(y[i]))))\n",
    "    mindist.append(min((min(dist[i]),min(dist[i]))))\n",
    "    \n",
    "maxXY=max(maxTraj)\n",
    "maxdist=100\n",
    "minXY=min(minTraj)\n",
    "mindist=min(mindist)\n",
    "\n",
    "Xscaled=[]\n",
    "yscaled=[]\n",
    "distscaled=[]\n",
    "for i in range(len(X)):\n",
    "    Xscaled.append((X[i]-minXY)/(maxXY-minXY))\n",
    "    yscaled.append((y[i]-minXY)/(maxXY-minXY))\n",
    "    distscaled.append((np.array(dist[i])-mindist)/(maxdist-mindist))\n",
    "    \n",
    "\n",
    "Xscaled=np.array(Xscaled)\n",
    "yscaled=np.array(yscaled)\n",
    "distscaled=np.array(distscaled)\n",
    "\n",
    "#inverse normalization function using min max method\n",
    "def invNORM (ypred,y,maxXY,minXY):    # function to calculate RMSE based on inverse normalized\n",
    "    yinv = np.zeros(y.shape)\n",
    "    ypredinv=np.zeros(ypred.shape)\n",
    "    for i in range(len(y)):\n",
    "        pos = sum (n>=0 for n in y[i])                #position of the last actual value on y, not padded\n",
    "        yinv[i][:pos]= y[i][:pos] * (maxXY-minXY) + minXY\n",
    "        ypredinv[i][:pos] = ypred[i][:pos]* (maxXY-minXY) + minXY\n",
    "    \n",
    "    return ypredinv, yinv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding sequences\n",
    "#### padding sequences to have same length by adding negative values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaled = pad_sequences(Xscaled, dtype='float32',value=-0.01)       \n",
    "o1 = pad_sequences(o1,dtype='float32',value=-0.01)\n",
    "o2 = pad_sequences(o2,dtype='float32',value=-0.01)\n",
    "o3 = pad_sequences(o3,dtype='float32',value=-0.01)\n",
    "distscaled = pad_sequences(distscaled,dtype='float32',value=-0.01)\n",
    "yscaled = pad_sequences(yscaled, padding='post',dtype='float32',value=-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging prepared data to create input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputseq=[]\n",
    "maxlen = Xscaled.shape[1]\n",
    "for i in range(Xscaled.shape[0]):\n",
    "    auxpadded = np.ones(maxlen)*(-0.01)\n",
    "    auxpadded[:14]=allauxdata[i]     #14 is the number of aux data\n",
    "    mrg_input=np.transpose(np.vstack((Xscaled[i],o1[i],o2[i],o3[i],distscaled[i],auxpadded)))\n",
    "    inputseq.append(mrg_input)\n",
    "\n",
    "inputseq=np.array(inputseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating test set before any modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate validation data and test set\n",
    "tst=0.2                 #% if test data\n",
    "#val=0.2                 #% if valid data       \n",
    "tstsize = int(np.floor(len(X) * tst))\n",
    "#valsize = int(np.floor(len(X) * val))\n",
    "\n",
    "\n",
    "tstlabel = random.sample(range(0, (inputseq.shape[0]-1)), len(range(tstsize)))\n",
    "inputseqTEST=inputseq[tstlabel]\n",
    "ytest=yscaled[tstlabel]\n",
    "\n",
    "trnlabel=[i for i in range(inputseq.shape[0]) if i not in tstlabel]\n",
    "inputseqtrain = inputseq[trnlabel]          #Excluding test set\n",
    "ytrain = yscaled[trnlabel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaLSTM(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, nodes=50, batch_size=32,\n",
    "                 epochs=200, Llayers=1, features=5, steps_in=83, steps_out=147\n",
    "                 ):\n",
    "\n",
    "        self.nodes = nodes\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.Llayers= Llayers\n",
    "        self.features = features\n",
    "        self.steps_in = steps_in\n",
    "        self.steps_out = steps_out\n",
    "        self.maxXY=654.7\n",
    "        self.minXY=650.1\n",
    "\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Masking(mask_value=-0.01, input_shape=(self.steps_in, self.features)))\n",
    "        for i in range(self.Llayers-1):\n",
    "            model.add(LSTM(self.nodes, activation='relu', return_sequences=True))\n",
    "        model.add(LSTM(self.nodes, activation='relu'))\n",
    "        model.add(Dense(self.steps_out))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def fit(self,inputseq,y):\n",
    "        start = time.time()\n",
    "        X = inputseq[:,:,:self.features]\n",
    "        self.model=self.create_model()\n",
    "        self.model.fit(X, y, epochs = self.epochs, verbose = 0, batch_size = self.batch_size)\n",
    "        end = time.time()\n",
    "        print (\"Finished Fitting Model. # of Epochs: %d\\n Time Taken : %d secs\"\n",
    "               % (self.epochs,end - start))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, inputseq):\n",
    "        X = inputseq[:,:,:self.features]\n",
    "        \n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self,inputseq,y):\n",
    "        \n",
    "        X = inputseq[:,:,:self.features]\n",
    "        ypred = self.predict(inputseq)\n",
    "        yinv = np.zeros(y.shape)\n",
    "        ypredinv = np.zeros(ypred.shape)\n",
    "        for i in range(len(y)):\n",
    "            pos = sum (n>=0 for n in y[i])                #position of the last actual value on y, not padded\n",
    "            yinv[i][:pos]= y[i][:pos] * (self.maxXY-self.minXY) + self.minXY\n",
    "            ypredinv[i][:pos] = ypred[i][:pos]* (self.maxXY-self.minXY) + self.minXY\n",
    "            \n",
    "        rmseScore = -sqrt(mean_squared_error(ypredinv, yinv))    #negative of RMSE\n",
    "\n",
    "        return rmseScore\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting Vanilla LSTM, using grid search for hypperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = (inputseq.shape[2]-1)\n",
    "n_steps_in=inputseq.shape[1]\n",
    "n_steps_out=ytrain.shape[1]\n",
    "\n",
    "VLmodel=VanillaLSTM(epochs=1, features=n_features,\n",
    "                    steps_in=n_steps_in, steps_out=n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {\"Llayers\" : [2,3]}#{\"nodes\" : [10,50,100],\"batch_size\": [32,64,128] ,\"Llayers\" : [1,2,3]}\n",
    "\n",
    "gs = GridSearchCV(VLmodel, tuned_params,cv = 8, refit= True, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result=gs.fit(inputseqtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/arash/ProjectVR/models/GridResults/VLSTM400_8_50_32_L.npy',grid_result.cv_results_)      \n",
    "#filename: ModelnameEpochs_CV_Nodes_Batch_changingparameters (L:LLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestVLSTM = grid_result.best_estimator_\n",
    "\n",
    "joblib.dump(bestVLSTM, '/home/arash/ProjectVR/models/trainedmodels/VLSTM1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestVLSTM.score(inputseqTEST,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bestVLSTM.model.history.history\n",
    "# plot history\n",
    "plt.plot(history['loss'], label='train')\n",
    "#pyplot.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm model with aux variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxLSTM(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, nodes=50, batch_size=32,\n",
    "                 epochs=200, Dlayers=1, Llayers=1, features=5, steps_in=83, steps_out=147\n",
    "                 ):\n",
    "\n",
    "        self.nodes = nodes\n",
    "        self.features = features\n",
    "        self.steps_in = steps_in\n",
    "        self.steps_out = steps_out\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.Dlayers= Dlayers   #number of hidden dense layers\n",
    "        self.Llayers = Llayers  #number of lstm layers\n",
    "        self.maxXY=654.7\n",
    "        self.minXY=650.1\n",
    "\n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        seq_input = Input(shape=(self.steps_in, self.features), dtype='float32', name='seq_input')\n",
    "        mask = Masking(mask_value=-0.01)(seq_input)\n",
    "        if self.Llayers==1:\n",
    "            \n",
    "            lstm_out = LSTM(self.nodes, activation='relu')(mask)\n",
    "            \n",
    "        else:\n",
    "            lstm_out = LSTM(self.nodes,activation='relu',return_sequences=True)(mask)\n",
    "            \n",
    "            for i in range(self.Llayers-2):\n",
    "                lstm_out = LSTM(self.nodes,activation='relu',return_sequences=True)(lstm_out)\n",
    "                \n",
    "            lstm_out = LSTM(self.nodes, activation='relu')(lstm_out)\n",
    "\n",
    "        #output for lstm, corresponds to 0.2 of loss, used to smooth training and regularization:\n",
    "        auxiliary_output = Dense(self.steps_out, activation='sigmoid' ,name='aux_output')(lstm_out)   \n",
    "\n",
    "        auxiliary_input = Input(shape=(14,), name='aux_input')\n",
    "        x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "\n",
    "        # We stack a deep densely-connected network on top\n",
    "        for i in range(self.Dlayers):\n",
    "            x = Dense(self.nodes, activation='relu')(x)\n",
    "\n",
    "        # And finally we add the main logistic regression layer\n",
    "        main_output = Dense(self.steps_out, activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "        modelaux = Model(inputs=[seq_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "\n",
    "\n",
    "        modelaux.compile(optimizer='adam', loss='mse',\n",
    "                      loss_weights=[1., 0.2])\n",
    "        return modelaux\n",
    "    \n",
    "    def fit(self,inputseq,y):\n",
    "        start = time.time()\n",
    "        X = inputseq[:,:,:self.features]\n",
    "        aux=inputseq[:,:,self.features][:,:14]      \n",
    "        self.model=self.create_model()\n",
    "        self.model.fit([X,aux], [y,y], epochs = self.epochs, verbose = 0, batch_size = self.batch_size)\n",
    "        end = time.time()\n",
    "        print (\"Finished Fitting AuxModel. # of Epochs: %d\\n Time Taken : %d secs\"\n",
    "               % (self.epochs,end - start))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, inputseq):\n",
    "        X = inputseq[:,:,:self.features]\n",
    "        aux=inputseq[:,:,self.features][:,:14]\n",
    "        \n",
    "        return self.model.predict([X,aux])\n",
    "    \n",
    "    def score(self,inputseq,y):\n",
    "        X = inputseq[:,:,:self.features]\n",
    "        aux = inputseq[:,:,self.features][:,:14]    \n",
    "        ypred = self.predict(inputseq)[0]        #auxlstm has two identical outputs, 1st in chosen\n",
    "        yinv = np.zeros(y.shape)\n",
    "        ypredinv = np.zeros(ypred.shape)\n",
    "        for i in range(len(y)):\n",
    "            pos = sum (n>=0 for n in y[i])                #position of the last actual value on y, not padded\n",
    "            yinv[i][:pos]= y[i][:pos] * (self.maxXY-self.minXY) + self.minXY\n",
    "            ypredinv[i][:pos] = ypred[i][:pos]* (self.maxXY-self.minXY) + self.minXY\n",
    "            \n",
    "        rmseScore = -sqrt(mean_squared_error(ypredinv, yinv))    #negative of RMSE\n",
    "        return rmseScore\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = (inputseq.shape[2]-1)\n",
    "n_steps_in=inputseq.shape[1]\n",
    "n_steps_out=ytrain.shape[1]\n",
    "\n",
    "Auxmodel=AuxLSTM(epochs=200, features=n_features,\n",
    "                    steps_in=n_steps_in, steps_out=n_steps_out)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {\"Dlayers\" : [1,2,3], \"Llayers\" : [2,3], 'nodes': [50,100]}\n",
    "\n",
    "gsaux = GridSearchCV(Auxmodel, tuned_params,cv = 8, refit= True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridaux_result=gsaux.fit(inputseqtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/arash/ProjectVR/models/GridResults/AuxLSTM200_8_DLB.npy',gridaux_result.cv_results_)\n",
    "bestauxLSTM = gridaux_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(bestauxLSTM, '/home/arash/ProjectVR/models/trainedmodels/AuxLSTM1.pkl')\n",
    "bestauxLSTM.score(inputseqTEST,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bestauxLSTM.model.history.history\n",
    "# plot history\n",
    "plt.plot(history['loss'], label='train')\n",
    "#pyplot.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing model performac for RMSE of the validation set, \n",
    "### all models are trained on same training and validation set, over 200 epochs,\n",
    "###### (Dense mentioned in models are hidden layers, output dense layer not counted\n",
    "* **Vanilla 1**: Batch size=32, LSTM units=50,No Masking input, no sample weights for output, two LSTM layer, loss=mse, **RMSE=~71** \n",
    "\n",
    "* **Vanilla 2:** Batch size=32, LSTM units=50, Masking input, no sample weights for output, two LSTM layer, loss=mse, **VAL RMSE=~0.29, Test RMSE=0.684**\n",
    "\n",
    "* **Vanilla 3:** Batch size=32, LSTM units=50, Masking input, -0.01 padded in order to not be considered in RMSE, two LSTM layer, loss=mse, **VAL RMSE=0.302, Test RMSE=0.546**\n",
    "\n",
    "\n",
    "\n",
    "* **AuxLSTM 1:** Batch size=32, LSTM units=50, Masking input, no sample weights for output, two LSTM layers, , one hidden dense, loss=mse, **VAL RMSE=~0.197, Test RMSE=0.616**\n",
    "\n",
    "* **AuxLSTM 2:** Batch size=32, LSTM units=50, Masking input, -0.01 padded in order to not be considered in RMSE, two LSTM layers, , one hidden dense, loss=mse, **VAL RMSE=~0.213, Test RMSE=0.538**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridaux_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "besttrain=AuxLSTM(Dlayers=2, Llayers=2, batch_size=32, epochs=1000, features=5, nodes=100,\n",
    "        steps_in=83, steps_out=147).fit(inputseqtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## history = besttrain.model.history.history\n",
    "# plot history\n",
    "plt.plot(history['loss'], label='train')\n",
    "#pyplot.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=besttrain.score(inputseqtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
