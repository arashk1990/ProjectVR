{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/arash/VRdataCleaning/DeepSurv/')\n",
    "\n",
    "import importlib\n",
    "import deepsurv\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "import argparse\n",
    "import uuid\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import optunity\n",
    "\n",
    "import logging\n",
    "from logging import handlers\n",
    "\n",
    "\n",
    "importlib.reload(deepsurv)\n",
    "\n",
    "from deepsurv import deep_surv, utils, viz\n",
    "\n",
    "from deepsurv.deepsurv_logger import DeepSurvLogger, TensorboardLogger\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('ALLDATA.pkl')\n",
    "data=data.loc[-data['Wait Time (s)'].isin(['Err1','Err2','Err3','Err4','Err5'])]\n",
    "data=data.loc[data['Age_9-12'].isin([0,1])]\n",
    "\n",
    "\n",
    "data=data.loc[:,['Wait Time (s)','PET (s)','Distace to Collision Point','Speed Limit', 'Lane Width', 'Minimum Gap', 'Mean Arrival Rate', 'AV', \n",
    "                   'Full Braking Before Impact_-1.0', 'Full Braking Before Impact_1', \n",
    "                   'Full Braking Before Impact_2', 'Full Braking Before Impact_3', 'Snowy',\n",
    "                   'One way', 'two way', 'Two way with median', 'Night', 'numcars', \n",
    "                    'Age_9-12', 'Age_15-18', 'Age_12-15', 'Age_18 - 24', 'Age_25 - 29', 'Age_30 - 39', 'Age_40 - 49', 'Age_50 - 59', 'Age_60+', 'Gender_Female', 'Occupation_Employed', 'Occupation_Student', 'Occupation_Unemployed', \n",
    "                   'Occupation_kid', 'Education_Bachelors degree', 'Education_College/University student', \n",
    "                   'Education_Doctorate degree', 'Education_High school diploma', 'Education_Masters degree', \n",
    "                   'Education_Professional degree', 'driving license_Yes', 'mode_Bike', 'mode_Car',\n",
    "                   'mode_Public Transit', 'mode_Walking', 'workwalk_No', 'workwalk_Sometimes', 'workwalk_Yes', \n",
    "                   'shopwalk_No', 'shopwalk_Sometimes', 'shopwalk_Yes', 'shopwalk_kid', 'Vrexp_Yes', 'Heart_Currently',\n",
    "                   'Heart_Over the years', 'vision_Currently', \n",
    "                   'vision_Over the years', 'anxiety_Currently', 'anxiety_Over the years', 'Headaches_Currently',\n",
    "                   'Headaches_Over the years', 'dizziness_Over the years']]       #numwalk and VRexpnum removed because of some false inputs in the data should be fixed later\n",
    "\n",
    "\n",
    "data=data.apply(pd.to_numeric, errors='coerce')\n",
    "data['E']=1   #all pedestrians cross, so no right censored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_deepsurv_ds(df, event_col = 'E', time_col = 'Wait Time (s)'):\n",
    "    # Extract the event and time columns as numpy arrays\n",
    "    e = df[event_col].values.astype(numpy.int32)\n",
    "    t = df[time_col].values.astype(numpy.float32)\n",
    "\n",
    "    # Extract the patient's covariates as a numpy array\n",
    "    x_df = df.drop([event_col, time_col], axis = 1)\n",
    "    x = x_df.values.astype(numpy.float32)\n",
    "    \n",
    "    # Return the deep surv dataframe\n",
    "    return {\n",
    "        'x' : x,\n",
    "        'e' : e,\n",
    "        't' : t\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with specific hyper parameters\n",
    "hyperparams = {\n",
    "    'L2_reg': 3.0,\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0.02,\n",
    "    'hidden_layers_sizes': [90,90,90],\n",
    "    'learning_rate': 1e-05,\n",
    "    'lr_decay': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'n_in': 10,\n",
    "    'standardize': True\n",
    "}\n",
    "\n",
    "# Create an instance of DeepSurv using the hyperparams defined above\n",
    "model = deep_surv.DeepSurv(**hyperparams)\n",
    "\n",
    "\n",
    "\n",
    "experiment_name = 'Wait Time analysis'\n",
    "logdir = './logs/tensorboard/'\n",
    "logger = TensorboardLogger(experiment_name, logdir=logdir)\n",
    "\n",
    "# Now we train the model\n",
    "update_fn=lasagne.updates.nesterov_momentum # The type of optimizer to use. \\\n",
    "                                            # Check out http://lasagne.readthedocs.io/en/latest/modules/updates.html \\\n",
    "                                            # for other optimizers to use\n",
    "n_epochs = 2000\n",
    "\n",
    "# If you have validation data, you can add it as the second parameter to the function\n",
    "metrics = model.train(data, n_epochs=n_epochs, logger=logger, update_fn=update_fn)\n",
    "\n",
    "# Print the final metrics\n",
    "print('Train C-Index:', metrics['c-index'][-1])\n",
    "# print('Valid C-Index: ',metrics['valid_c-index'][-1])\n",
    "\n",
    "# Plot the training / validation curves\n",
    "viz.plot_log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_logger(logdir):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    format = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    \n",
    "    # Print to Stdout\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(format)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # Print to Log file\n",
    "    fh = logging.FileHandler(os.path.join(logdir, 'log_' + str(uuid.uuid4())))\n",
    "    fh.setFormatter(format)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_box_constraints(file):\n",
    "    with open(file, 'rb') as fp:\n",
    "        return json.loads(fp.read())\n",
    "\n",
    "def save_call_log(file, call_log):\n",
    "    with open(file, 'wb') as fp:\n",
    "        pickle.dump(call_log, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objective_function(num_epochs, logdir, update_fn = lasagne.updates.sgd):\n",
    "    '''\n",
    "    Returns the function for Optunity to optimize. The function returned by get_objective_function\n",
    "    takes the parameters: x_train, y_train, x_test, and y_test, and any additional kwargs to \n",
    "    use as hyper-parameters.\n",
    "\n",
    "    The objective function runs a DeepSurv model on the training data and evaluates it against the\n",
    "    test set for validation. The result of the function call is the validation concordance index \n",
    "    (which Optunity tries to optimize)\n",
    "    '''\n",
    "    def format_to_deepsurv(x, y):\n",
    "        return {\n",
    "            'x': x,\n",
    "            'e': y[:,0].astype(np.int32),\n",
    "            't': y[:,1].astype(np.float32)\n",
    "        }\n",
    "\n",
    "    def get_hyperparams(params):\n",
    "        hyperparams = {\n",
    "            'batch_norm': False,\n",
    "            'activation': 'selu',\n",
    "            'standardize': True\n",
    "        }\n",
    "        # @TODO add default parameters and only take necessary args from params\n",
    "        # protect from params including some other key\n",
    "\n",
    "        if 'num_layers' in params and 'num_nodes' in params:\n",
    "            params['hidden_layers_sizes'] = [int(params['num_nodes'])] * int(params['num_layers'])\n",
    "            del params['num_layers']\n",
    "            del params['num_nodes']\n",
    "\n",
    "        if 'learning_rate' in params:\n",
    "            params['learning_rate'] = 10 ** params['learning_rate']\n",
    "\n",
    "        hyperparams.update(params)\n",
    "        return hyperparams\n",
    "\n",
    "    def train_deepsurv(x_train, x_test,\n",
    "        **kwargs):\n",
    "        hyperparams = get_hyperparams(kwargs)\n",
    "        #select number of features\n",
    "        ReliefAvg = pd.read_pickle('/home/arash/VRdataCleaning/DeepSurv/deepsurv/ReliefAvg.pkl')\n",
    "        x_train=x_train[np.append(ReliefAvg.sort_values(['Importance'],ascending=False)\n",
    "                             ['Covariate'][0:n_in].values,np.array(['Wait Time (s)','E']))]\n",
    "        x_train = dataframe_to_deepsurv_ds(x_train, event_col = 'E', time_col= 'Wait Time (s)')\n",
    "        \n",
    "        x_test=x_test[np.append(ReliefAvg.sort_values(['Importance'],ascending=False)\n",
    "                             ['Covariate'][0:n_in].values,np.array(['Wait Time (s)','E']))]\n",
    "        x_test = dataframe_to_deepsurv_ds(x_test, event_col = 'E', time_col= 'Wait Time (s)')\n",
    "        \n",
    "        x_train = x_train['x']\n",
    "        e_train = x_train['e']\n",
    "        t_train = x_train['t']\n",
    "        y_train = np.column_stack((e_train, t_train))\n",
    "        \n",
    "        x_test=x_test['x']\n",
    "        e_test = x_test['e']\n",
    "        t_test = x_test['t']\n",
    "        y_test = np.column_stack((e_test, t_test))\n",
    "        \n",
    "        \n",
    "        # Standardize the datasets\n",
    "        train_mean = x_train.mean(axis = 0)\n",
    "        train_std = x_train.std(axis = 0)\n",
    "\n",
    "        x_train = (x_train - train_mean) / train_std\n",
    "        x_test = (x_test - train_mean) / train_std\n",
    "\n",
    "        train_data = format_to_deepsurv(x_train, y_train)\n",
    "        valid_data = format_to_deepsurv(x_test, y_test)\n",
    "\n",
    "        \n",
    "\n",
    "        # Set up Tensorboard loggers\n",
    "        # TODO improve the model_id for Tensorboard to better partition runs\n",
    "        model_id = str(hash(str(hyperparams)))\n",
    "        run_id = model_id + '_' + str(uuid.uuid4())\n",
    "        logger = TensorboardLogger('hyperparam_search', \n",
    "            os.path.join(logdir,\"tensor_logs\", model_id, run_id))\n",
    "\n",
    "        network = deep_surv.DeepSurv(**hyperparams)\n",
    "        metrics = network.train(train_data, n_epochs = num_epochs, logger=logger, \n",
    "            update_fn = update_fn, verbose = False)\n",
    "\n",
    "        result = network.get_concordance_index(**valid_data)\n",
    "        main_logger.info('Run id: %s | %s | C-Index: %f | Train Loss %f' % (run_id, str(hyperparams), result, metrics['loss'][-1][1]))\n",
    "        return result\n",
    "\n",
    "    return train_deepsurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "NUM_FOLDS = 3\n",
    "logdir='/home/arash/VRdataCleaning/logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    global main_logger\n",
    "main_logger = load_logger(logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    main_logger.debug('Loading dataset: ' + args.dataset)\n",
    "box_constraints = load_box_constraints('/home/arash/VRdataCleaning/box_constraints.0.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fxn = get_objective_function(NUM_EPOCHS, logdir, \n",
    "                                 utils.get_optimizer_from_str('adam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fxn = optunity.cross_validated(x=data, num_folds=NUM_FOLDS)(opt_fxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    main_logger.debug('Maximizing C-Index. Num_iterations: %d' % args.num_evals)\n",
    "opt_params, call_log, _ = optunity.maximize(opt_fxn, num_evals=10,\n",
    "        solver_name='sobol',\n",
    "        **box_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    main_logger.debug('Optimal Parameters: ' + str(opt_params))\n",
    "#    main_logger.debug('Saving Call log...')\n",
    "print(call_log._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_call_log(os.path.join(logdir, 'optunity_log_%s.pkl' % (str(uuid.uuid4()))), call_log._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
